distinct()
df_to_fill_bed <- df_to_fill_bed %>%
dplyr::filter(contig_n %in% keep_contigs$contig_n)
}
text <- df_to_fill_bed %>%
group_by(library) %>%
dplyr::mutate(mean=mean(coverage),
sd=sd(coverage),
zeros=NROW(which(coverage==0))) %>%
dplyr::select(library,mean,sd,zeros) %>%
distinct() %>%
dplyr::mutate(label=paste0("mean=",round(mean,2),
"\n sd=",round(sd,2),
"\n zero=", zeros))
p1 <- df_to_fill_bed %>%
group_by(library,coverage) %>%
tally() %>%
ggplot(., aes(x=coverage,y=n, color=library))+
geom_line() +
xlim(0,100) +
theme(legend.position="top",
legend.title = element_blank())+
labs(x="coverage per site",
y="Frequency (# sites)") +
theme_bw()+
theme(legend.position="none")
p2 <- df_to_fill_bed %>%
dplyr::filter(coverage < 4) %>%
ggplot(., aes(x=coverage, color=library)) +
geom_histogram(fill="white", alpha=0.5,
position="identity",
binwidth=0.5)+
facet_grid(rows = vars(library)) +
theme(legend.position="top",
legend.title = element_blank())+
xlim(-0.5,5.5) +
labs(x="coverage per site",
y="Frequency (# sites)") +
theme_bw()+
theme(legend.position="none",
strip.text.y = element_text(size = 7,
colour = "black",
angle = 0)) +
geom_text(
data    = text,
mapping = aes(x = Inf, y = Inf, label = label, hjust=1.0, vjust=1), #vjust=1 hjust=1.0
size=3
)
# final cov plot
cov_plot <- ggarrange(p1, p2,
ncol=2, nrow=1, common.legend = TRUE,
labels=c("A","B"))
########################################
# display correlation by coverage:
bed_corr <- df_to_fill_bed
head(bed_corr)
bed_corr <- bed_corr %>%
dplyr::select(contig,position,library,coverage) %>%
pivot_wider(names_from=library, values_from=coverage, values_fill = 0)
tail(bed_corr)
bed_corr_libs <- bed_corr[,3:ncol(bed_corr)]
# compute correlation between libs based on their coverage (all contigs)
# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
ut <- upper.tri(cormat)
data.frame(
row = rownames(cormat)[row(cormat)[ut]],
column = rownames(cormat)[col(cormat)[ut]],
cor  =(cormat)[ut],
p = pmat[ut]
)
}
res <- rcorr(as.matrix(bed_corr_libs), type = "pearson")
corr_df <- flattenCorrMatrix(res$r, res$P) %>%
arrange(desc(cor))
fwrite(x=corr_df, file=paste0(mydir,paste0(species,"_",goal,"_coverage_correlation.csv")))
########################################
# display top largest contigs' lowest coverage areas
# keep n largest contigs
if (species=="Pa") {
keep_largest_contigs <- df_to_fill_bed %>%
group_by(contig_n) %>%
dplyr::summarize(max=max(position)) %>%
dplyr::filter(max>=500000) %>%
dplyr::select(contig_n) %>%
distinct()
df_to_fill_bed <- df_to_fill_bed %>%
dplyr::filter(contig_n %in% keep_largest_contigs$contig_n)
}
mycolors<- c('#999999', # grey
'#E69F00', # orange
'#56B4E9', # blue
"#FF3300", # red
"#333399", # purple
"#33CC33", # green
"#FFFF00") # yellow
myshapes<- c(0, 1, 8, 9, 15, 17, 19)
lowest_cov_plot <- df_to_fill_bed %>%
dplyr::filter(coverage < 10) %>%
ggplot(., aes(x=position,y=coverage,group=library)) +
geom_point(aes(shape=library, color=library), size=0.01)+
scale_shape_manual(values=myshapes)+
scale_color_manual(values=mycolors)+
#facet_grid(rows = vars(contig)) +
#facet_grid(rows = contig, scales="free_x") +
facet_grid(library~contig) +
theme_bw()+
theme(axis.text.x=element_text(size=6, angle=90),
strip.text.y = element_text(size = 8,
colour = "black",
angle = 0),
legend.position="right")+
labs(x="Genomic position (bp)",
y="Coverage",
title = "Lowest coverage regions (4 largest contigs)")
# check sites with zero coverage: do the genomic positions overlap?
x0 <- df_to_fill_bed %>%
dplyr::filter(coverage < 1) %>%
dplyr::mutate(library=as.factor(recode_fun(library))) %>%
dplyr::filter(library %in% my_subset) %>%
drop.levels()
multiple_DFs <- split(x0, list(x0$contig, x0$library), drop = TRUE)
# empty df
lowcovDF <- data.frame(contig=character(),
start_position=numeric(),
end_position=numeric(),
library=character())
for (single_DF in multiple_DFs) {
single <- as.data.frame(single_DF)
lib <- unique(single$library)
x0 <- single %>%
dplyr::arrange(position)
x0$bins <- c(0, cumsum(diff(x0$position) != 1))
x0 <- x0 %>%
drop.levels() %>%
group_by(contig,bins) %>%
dplyr::summarise(start_position=min(position),
end_position=max(position), .groups='drop') %>%
dplyr::select(contig,start_position,end_position)
x0$library <- paste0(lib)
lowcovDF <- rbind(lowcovDF,x0)
}
lowcovDF <- lowcovDF %>%
dplyr::arrange(contig,start_position) %>%
dplyr::mutate(length_bp=end_position-start_position)
# for manuscript:
lowcovDF %>%
group_by(library) %>%
dplyr::summarise(sum=sum(length_bp))
fwrite(x=lowcovDF, file=paste0(mydir,paste0(species,"_",goal,"_zero_cov_sites.csv")))
########################################
########################################
# Base quality stats (reads from bam - BQ computed with ALFRED)
BQ_files = list.files(mydir,pattern="^BQ")
datalist = list()
for (BQ_file in BQ_files) {
# open file contaning all the PHRED scores:
BQ_df <- read_delim(file.path(mydir,BQ_file),
"\t", escape_double = FALSE, trim_ws = TRUE)
head(BQ_df)
id <- BQ_file
id <- sub(".*interleaved_", "",id)
id <- gsub(".dedup.tsv.tsv","",id)
id <- recode_fun(id)
BQ_df$library <- paste0(id)
BQ_df <- BQ_df %>%
dplyr::select(Position,BaseQual,Read,library)
# subset
BQ_df <- BQ_df %>%
dplyr::filter(library %in% my_subset) %>%
drop.levels()
datalist[[BQ_file]] <- BQ_df # add it to your list
}
BQ_data = do.call(rbind, datalist)
bq_data_stats <- BQ_data %>%
group_by(library, Read) %>%
dplyr::summarise(median_BQ=median(BaseQual),
mean_BQ=mean(BaseQual),
sd_BQ=min(BaseQual),
bp_count=n())
fwrite(x=bq_data_stats, file=paste0(mydir,paste0(species,"_",goal,"_base_quality.csv")))
########################################
########################################
# RL content:
rl_files = list.files(mydir,pattern="^RL")
# construct an empty dataframe to build on
rl_to_fill <- data.frame(
library = character(),
Readlength = numeric(),
Count = numeric(),
Fraction = numeric(),
Read = character(),
stringsAsFactors = FALSE
)
for (rl_file in rl_files) {
# read in file
rl_df <- read.table(file.path(mydir,rl_file), quote="\"", comment.char="", header = TRUE)
unique(rl_df$Library)
head(rl_df)
rl_df <- rl_df %>%
dplyr::select(Sample,Readlength,Count, Fraction, Read)
rl_df$library <- str_replace_all(rl_df$Sample, "reduced_trimmed2_trimmed_interleaved_", "")
rl_df$library <- gsub(".dedup","",rl_df$library)
rl_df$library <- recode_fun(rl_df$library)
rl_df$Sample <- NULL
rl_to_fill <- rbind(rl_to_fill, rl_df)
}
# subset
rl_to_fill <- rl_to_fill %>% dplyr::filter(library %in% my_subset)
# re-order libs
rl_to_fill$library <- as.factor(rl_to_fill$library)
rl_to_fill <- reorder_lib_fun(rl_to_fill)
# expand rows based on count (to create a density plot, like the insert size one)
#rl_to_fill_expand <- setDT(expandRows(rl_to_fill, "Count"))
#head(rl_to_fill_expand)
# plot, keep both reads
RL_plot <- ggplot(rl_to_fill, aes(x=Readlength, y=Count,colour=Read, fill=Read)) +
scale_x_continuous(limits=c(100,310)) +
xlab("length of mapped reads (bp)")+
geom_bar(alpha=0.5, stat="identity", width = 0.001)+
facet_grid(rows = vars(library), scales = "free") +
theme_bw() +
theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line = element_line(colour = "black"),
legend.title = element_blank(),
legend.position = "top",
axis.text=element_text(size=8),
axis.title=element_text(size=14),
strip.text.y = element_text(size = 8,
colour = "black",
angle = 0))
########################################
########################################
# ME data (Alfred output):
me_files = list.files(mydir,pattern="^ME")
myDF <- data.frame(me_files, stringsAsFactors = F)
MElist = list()
for (row in 1:nrow(myDF)) {
me_file <- myDF[row,1]
# read in file
me_df <- read.table(file.path(mydir,me_file), quote="\"", comment.char="", header = TRUE)
me_df$Sample <- sub(".*interleaved_", "",me_df$Sample)
me_df$Sample <- gsub(".dedup", "",me_df$Sample)
me_df <- me_df %>%
dplyr::mutate(library=as.factor(recode_fun(Sample))) %>%
select(library, everything()) # bring at first position
MElist[[row]] <- me_df # add it to your list
}
ME_data = do.call(rbind, MElist)
# subset
ME_data <- ME_data %>% dplyr::filter(library %in% my_subset)
# re-order libs
ME_data$library <- as.factor(ME_data$library)
ME_data <- reorder_lib_fun(ME_data)
# store names
n <- ME_data$Sample
# transpose all but the first column (name)
ME_data <- as.data.frame(t(ME_data[,-2]))
colnames(ME_data) <- n
ME_data$Sample <- factor(row.names(ME_data))
rownames(ME_data) <- NULL
# # ME data save as png :
# kbl(ME_data, format = "html") %>%
#   kable_classic() %>%
#   kable_styling(font_size = 15, "striped") %>%
#   kableExtra::as_image(file=paste0(mydir,species,"_ALFRED_data.png"))
# Save as Table :
fwrite(x=ME_data, file=paste0(mydir,paste0(species,"_",goal,"_mapping.csv")))
########################################
########################################
# Contamination:
kraken_files = list.files(mydir,pattern="^kraken_")
# construct an empty dataframe to build on
df_to_fill_kraken <- data.frame(
library = character(),
read_count = numeric(),
percentage_of_tot_unmapped_reads = numeric(),
species = character(),
stringsAsFactors = FALSE
)
for (kraken_file in kraken_files) {
kra <- read_delim(file.path(mydir,kraken_file),"\t",
escape_double = FALSE,
col_names = FALSE, trim_ws = TRUE)
if (NROW(kra)>1) {
id <- kraken_file
id <- sub(".*interleaved_", "", id)
id <- gsub(".dedup","",id)
id <- recode_fun(id)
kra$library=paste0(as.character(id))
# The output of kraken-report is tab-delimited, with one line per taxon. The fields of the output, from left-to-right, are as follows:
#   1. Percentage of reads covered by the clade rooted at this taxon
# 2. Number of reads covered by the clade rooted at this taxon
# 3. Number of reads assigned directly to this taxon
# 4. A rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. All other ranks are simply '-'.
# 5. NCBI taxonomy ID
# 6. indented scientific name
kra_filtered <- kra %>%
dplyr::filter(X3>0) %>%
dplyr::mutate(sum_reads=sum(X3)) %>%
dplyr::mutate(percentage_of_tot_unmapped_reads=(X3/sum_reads)*100) %>%
dplyr::select(library,X3,percentage_of_tot_unmapped_reads,X6)
colnames(kra_filtered) <- c("library","read_count","percentage_of_tot_unmapped_reads", "species")
df_to_fill_kraken <- rbind(
df_to_fill_kraken,
kra_filtered
)
}
}
# subset
df_to_fill_kraken <- df_to_fill_kraken %>% dplyr::filter(library %in% my_subset)
# re-order libs
df_to_fill_kraken$library <- as.factor(df_to_fill_kraken$library)
df_to_fill_kraken <- reorder_lib_fun(df_to_fill_kraken)
# Save as Table :
fwrite(x=df_to_fill_kraken, file=paste0(mydir,species,"_",goal,"_kraken.csv"))
########################################
########################################
# Assembly: comparison of HF vs HF_0.6x ( double left clean up )
# if this is "goal_size_selection" read in the assembly stats, parse and save
if (grepl("size_selection", mydir, fixed = TRUE)==TRUE) {
print ("We look at assembly stats, as these two libs are compared to assess the effect of size selection on assembly")
assembly_stats <- read_delim(paste0(mydir,"assembly_stats.txt"),
"\t", escape_double = FALSE, trim_ws = TRUE, skip = 1)
id <- assembly_stats$filename
id <- sub(".*interleaved_", "", id)
id <- gsub("/contigs.fa","",id)
id
id <- recode_fun(id)
assembly_stats$library=paste0(as.character(id))
assembly_stats <- assembly_stats %>%
dplyr::select(library, everything())
# Save as Table :
fwrite(x=assembly_stats, file=paste0(mydir,species,"_",goal,"_assembly.csv"))
}
################################################################################
################################################################################
################################################################################
# plot
pdf(paste0(mydir,species,"_",goal,'_out.pdf'))
# plot coverage
cov_plot
# print lowest coverage regions
lowest_cov_plot
# plot insert size
insert_size_plot
#straight_GC
GC_picard
# plot PHRED scores
PHRED_plot
# plot read lengths
RL_plot
dev.off()
################################################################################
################################################################################
# concatenating created pdfs:
# setwd(mydir)
# ## Setwd and Collect the names of the figures to be glued together
# ff <- dir(pattern=".pdf")
# ## The name of the pdf doc that will contain all the figures
# outFileName <- paste0(species,"_all.pdf")
#
# ## Make a system call to pdftk
# system2(command = "pdftk",
#         args = c(shQuote(ff), "cat output", shQuote(outFileName)))
################################################################################
################################################################################
# load libs
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(readr)
library(ggpubr)
library(weights)
library(readr)
library(splitstackshape)
library(stringr)
library(corrr)
library(data.table)
library(stringr)
library(readxl)
library(reshape2)
library(textshape)
# set directories and select samples:
barcode_libs <- "~/Desktop/MG1655/goal_barcode/"
barcode_source_data <- "~/Desktop/MG1655/goal_barcode/source_data/"
phred_dir <- "~/Desktop/MG1655/raw_libs/"
IS_files <- grep(list.files(barcode_libs,pattern="^picardIS"),
pattern='.txt', value=TRUE)
# construct an empty dataframe to build on
df_to_fill_insert_size <- data.frame(
insert_size = numeric(),
library = character(),
stringsAsFactors = FALSE
)
for (IS_file in IS_files) {
# read in file
IS_df <- read_delim(file.path(barcode_libs,IS_file), "\t", escape_double = FALSE, trim_ws = TRUE, skip = 10)
IS_df <- IS_df %>%
uncount(All_Reads.fr_count)
id <- sub(".*interleaved_", "", IS_file)
id <- gsub(".dedup.txt","",id)
IS_df$library=paste0(as.character(id))
df_to_fill_insert_size <- rbind(
df_to_fill_insert_size,
IS_df
)
}
# expand rows based on Count, compute median insert sizes
med_IS <- df_to_fill_insert_size %>%
group_by(library) %>%
dplyr::summarize(median=round(median(insert_size),2),
mean=round(mean(insert_size),2),
sd=round(sd(insert_size),2),
var=round(var(insert_size),2))
# for manuscript:
mean(med_IS$mean)
sd(med_IS$mean)
hist(med_IS$mean)
med_IS
# for manuscript:
mean(med_IS$mean)
min(med_IS$mean)
max(med_IS$mean)
min(med_IS$sd)
max(med_IS$sd)
# load libs
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(readr)
library(ggpubr)
library(weights)
library(readr)
library(splitstackshape)
library(stringr)
library(corrr)
library(data.table)
library(stringr)
library(readxl)
library(reshape2)
library(textshape)
# set directories and select samples:
barcode_libs <- "~/Desktop/MG1655/goal_barcode/"
barcode_source_data <- "~/Desktop/MG1655/goal_barcode/source_data/"
phred_dir <- "~/Desktop/MG1655/raw_libs/"
demux_clean <- read_csv(paste0(barcode_source_data, "demux_clean.tsv"), col_names = FALSE)
demux_clean <- demux_clean[6:677,]
demux_clean$seq <- rep(seq(1,7), 96)
demux_clean <- as.data.frame(demux_clean)
counts <- demux_clean %>%
dplyr::filter(seq=="5") # filter barcode counts (all)
counts$X1 <- gsub("</BarcodeCount>","",counts$X1)
counts$X1 <- gsub("<BarcodeCount>","",counts$X1)
counts <- as.data.frame(as.numeric(counts$X1))
colnames(counts) <- "counts"
TOT_BARCODE_COUNTS <- sum(counts$counts)
sort(counts$counts)
head(counts)
counts$counts<45000
a <- counts %>%
dplyr::summarise(barcodes_combos=n(),
Min=min(counts),
Mean=mean(counts),
Median=median(counts),
Sd=sd(counts),
Max=max(counts),
Sum=sum(counts),
Coeff.variation=sd(counts)/mean(counts))
a
counts$counts<45000
counts
#Price comparison:
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)
library(ggpubr)
setwd("~/Hackflex/source_data")
out_dir <- "~/Desktop/MG1655/"
price_per_samples_to_R <- read_csv("price_per_sample.csv")
head(price_per_samples_to_R)
plot_fold1 <- price_per_samples_to_R %>%
dplyr::select(samples,method,`price AUD`) %>%
pivot_wider(names_from=method,values_from=`price AUD`) %>%
dplyr::mutate(`Standard Flex vs Hackflex`=Hackflex/`Nextera Flex`,
`Standard Flex vs Hackflex 1/2 polymerase`=`Hackflex 1/2 polymerase`/`Nextera Flex`) %>%
dplyr::select(samples,`Standard Flex vs Hackflex`,`Standard Flex vs Hackflex 1/2 polymerase`) %>%
pivot_longer(cols=c(`Standard Flex vs Hackflex`,
`Standard Flex vs Hackflex 1/2 polymerase`), values_to="fold_diff", names_to="comparison") %>%
ggplot(., aes(x=samples, y=fold_diff, fill=comparison, color=comparison))+
geom_point(size=1,aes(shape = comparison)) +
scale_shape_manual(values=c(3, 17)) +
geom_line() +
labs(x="samples",
y="price fold difference") +
theme_bw()+
theme(legend.title = element_blank(),
legend.position="top",
axis.title=element_text(size=18),
axis.text=element_text(size=14))+
xlim(0,5010)+
ylim(0,1)
plot_fold1
